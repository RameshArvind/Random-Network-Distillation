{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f349cd696eae4548ae81876018b87fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_366bfab6750e4f1580598b3bb1d62f97",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0a3c189d8f89480eb7d77730d9512c1c",
              "IPY_MODEL_c79f4606d482406bb8a2cec42b0c6498"
            ]
          }
        },
        "366bfab6750e4f1580598b3bb1d62f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a3c189d8f89480eb7d77730d9512c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33aea79bb3354326968bc76ecd5eea4b",
            "_dom_classes": [],
            "description": "  2%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 3000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 61,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3762d32f146b4726830f85ac166d021a"
          }
        },
        "c79f4606d482406bb8a2cec42b0c6498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1568a33943a14ea997d35db3427b62b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 61/3000 [3:14:59&lt;156:50:16, 192.11s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a55a1a99d454e92b5be5e0f1f142de0"
          }
        },
        "33aea79bb3354326968bc76ecd5eea4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3762d32f146b4726830f85ac166d021a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1568a33943a14ea997d35db3427b62b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a55a1a99d454e92b5be5e0f1f142de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSGtnwI5Ft8y",
        "colab_type": "text"
      },
      "source": [
        "# RND reference [https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/RND%20Montezuma's%20revenge%20PyTorch/agents.py](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/RND%20Montezuma's%20revenge%20PyTorch/agents.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TNCycMMLb5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines==2.5.1 box2d box2d-kengz\n",
        "!pip install torchcontrib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjr31ShbKop0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "# import torchcontrib\n",
        "# from torchcontrib.optim import SWA\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import gym\n",
        "from IPython import display as ipythondisplay\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTP2gmkT8CKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNDBaseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNDBaseNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 128, 3)\n",
        "        self.conv2 = nn.Conv2d(128, 256, 3)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.head = nn.Linear(256, 256)\n",
        "\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.kaiming_normal_(p.weight, init.calculate_gain('leaky_relu'))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.kaiming_normal_(p.weight, init.calculate_gain('leaky_relu'))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = self.avg_pool(x).reshape(x.shape[0], -1)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wRl98W7818Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNDNetwork, self).__init__()\n",
        "        self.target = RNDBaseNetwork()\n",
        "        self.predictor =  RNDBaseNetwork()\n",
        "\n",
        "        for param in self.target.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def forward(self, x):\n",
        "        predicted, target = self.predictor(x), self.target(x)\n",
        "        return predicted, target\n",
        "    \n",
        "    def get_intrinsic_reward(self, state):\n",
        "      predicted, target = self.predictor(state), self.target(state)\n",
        "      intrinsic_reward = (predicted - target).pow(2).sum() / 2\n",
        "      return intrinsic_reward.clamp(-5, 5).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT8DeJj1Lgop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size, hidden_size = 256):\n",
        "\n",
        "    super(PolicyNetwork, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 3)\n",
        "    self.conv2 = nn.Conv2d(64, hidden_size, 3)\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.actor_network = nn.Linear(hidden_size, action_size)\n",
        "    self.critic_network_extrinsic = nn.Linear(hidden_size, 1)\n",
        "    self.critic_network_intrinsic = nn.Linear(hidden_size, 1)\n",
        "    self.num_actions = action_size\n",
        "    self.rnd_network = RNDNetwork()\n",
        "\n",
        "    for p in self.modules():\n",
        "        if isinstance(p, nn.Conv2d):\n",
        "            init.kaiming_normal_(p.weight, init.calculate_gain('leaky_relu'))\n",
        "            p.bias.data.zero_()\n",
        "\n",
        "        if isinstance(p, nn.Linear):\n",
        "            init.kaiming_normal_(p.weight, init.calculate_gain('leaky_relu'))\n",
        "            p.bias.data.zero_()\n",
        "\n",
        "  def get_intrinsic_reward(self, state):\n",
        "      target_features, predictor_features = self.rnd_network(state)\n",
        "      intrinsic_reward = (target_next_feature - predict_next_feature).pow(2).sum() / 2\n",
        "      return intrinsic_reward.item()\n",
        "\n",
        "  def forward(self, state_representation):\n",
        "    x = state_representation\n",
        "    x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "    x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "    x = self.avg_pool(x).reshape(x.shape[0], -1)\n",
        "    # print(x.shape)\n",
        "    x = F.leaky_relu(self.fc1(x))\n",
        "    x = F.leaky_relu(self.fc2(x))\n",
        "    policy = self.actor_network(x)\n",
        "    policy = F.softmax(policy, dim = 1)\n",
        "    # x2 = F.leaky_relu(self.fc2(state_representation))\n",
        "    value_extrinsic = self.critic_network_extrinsic(x)\n",
        "    value_intrinsic = self.critic_network_intrinsic(x)\n",
        "    return value_extrinsic, value_intrinsic, policy\n",
        "\n",
        "  def get_action(self, state_representation):\n",
        "    _, _, policy = self.forward(state_representation)\n",
        "    # Get an action based on the probabilities returned from network\n",
        "    weighted_probability_chosen_action = np.random.choice(self.num_actions, p=np.squeeze(policy.cpu().detach().numpy()))\n",
        "    return weighted_probability_chosen_action\n",
        "\n",
        "  def get_probability_of_actions(self, states, actions):\n",
        "      values_extrinsic, values_intrinsic, policies = self.forward(states)\n",
        "      action_probs = policies.gather(1, actions.unsqueeze(1))\n",
        "      # Add small epsilon to prevent nans?\n",
        "      # https://www.reddit.com/r/reinforcementlearning/comments/8k54mz/my_ppo_agent_collapse_to_a_single_value/\n",
        "      entropy = -(policies * torch.log(policies + 1e-20)).sum(1)\n",
        "      return values_extrinsic.squeeze(), values_intrinsic.squeeze(), action_probs.squeeze(), entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYP39MwYNH7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Based on https://github.com/astooke/rlpyt/blob/master/rlpyt/algos/utils.py#L138\n",
        "def get_advantages_and_returns(rewards, values, bootstrap_value, done_list):\n",
        "    GAMMA = 0.99\n",
        "    LAMBDA = 1\n",
        "\n",
        "    advantages = torch.zeros(len(rewards)).cuda()\n",
        "    returns = torch.zeros(len(rewards)).cuda()\n",
        "    not_done = 1 - done_list\n",
        "    advantages[-1] = rewards[-1] + GAMMA * bootstrap_value * not_done[-1] - values[-1]\n",
        "\n",
        "    for i in reversed(range(len(rewards) - 1)):\n",
        "        delta = rewards[i] + GAMMA * values[i + 1] * not_done[i] - values[i]\n",
        "        advantages[i] = delta + GAMMA * LAMBDA * advantages[i + 1] * not_done[i]\n",
        "\n",
        "    # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-20)\n",
        "\n",
        "    returns[:] = advantages + values\n",
        "\n",
        "    return advantages, returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ3rnmw2YnK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discounted_rewards(rewards, bootstrap_value, done_list):\n",
        "    GAMMA = 0.99\n",
        "\n",
        "    discounted_rewards = torch.Tensor(len(rewards)).float()\n",
        "    not_done = 1 - done_list\n",
        "\n",
        "    discounted_rewards[-1] = rewards[-1] + GAMMA * bootstrap_value * not_done[-1]\n",
        "    for i in reversed(range(len(rewards) - 1)):\n",
        "        discounted_rewards[i] = rewards[i] + GAMMA * discounted_rewards[i+1] * not_done[i]\n",
        "\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-20)\n",
        "\n",
        "    return discounted_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXO-DsYtRBcG",
        "colab_type": "code",
        "outputId": "0d8c983d-4b6b-4a33-c8ca-dbed2735bd71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "done_list = np.array([False, False, False, True, False]).astype(int)\n",
        "rewards = torch.tensor([10, 20, 20, 0, 0]).float().cuda()\n",
        "bootstrap = 10\n",
        "values = torch.tensor([10, 20, 20, 0, 0]).float().cuda()\n",
        "get_advantages_and_returns(rewards, values, bootstrap, done_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([39.4020, 19.8000,  0.0000,  0.0000,  9.9000], device='cuda:0'),\n",
              " tensor([49.4020, 39.8000, 20.0000,  0.0000,  9.9000], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yal4yt16U9zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_ppo_update(states, next_states, actions, rewards_extrinsic, rewards_intrinsic, done_list, bootstrap_state, old_model, new_model, rnd_model, policy_optimizer, rnd_optimizer):\n",
        "    NUM_PPO_UPDATES = 5\n",
        "    RATIO_CLIP = 0.3\n",
        "\n",
        "    _, _, old_action_probs, _ = old_model.get_probability_of_actions(states, actions)\n",
        "    old_action_probs = old_action_probs.detach()\n",
        "\n",
        "    for _ in range(NUM_PPO_UPDATES):\n",
        "        boostrap_extrinsic_new_value, boostrap_intrinsic_new_value, _ = new_model(bootstrap_state)\n",
        "        boostrap_extrinsic_new_value = boostrap_extrinsic_new_value.squeeze()\n",
        "        boostrap_intrinsic_new_value = boostrap_intrinsic_new_value.squeeze()\n",
        "\n",
        "        new_values_extrinsic, new_values_intrinsic, new_action_probs, new_entropy = new_model.get_probability_of_actions(states, actions)\n",
        "\n",
        "        advantages_extrinsic, _ = get_advantages_and_returns(rewards_extrinsic, new_values_extrinsic, boostrap_extrinsic_new_value, done_list)\n",
        "        advantages_intrinsic, _ = get_advantages_and_returns(rewards_intrinsic, new_values_intrinsic, boostrap_intrinsic_new_value, done_list)\n",
        "\n",
        "        advantages = advantages_extrinsic + advantages_intrinsic\n",
        "\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-20)\n",
        "\n",
        "        ratios = new_action_probs / (old_action_probs + 1e-20)\n",
        "\n",
        "        surr1 = ratios * advantages.detach()\n",
        "        surr2 = torch.clamp(ratios, 1-RATIO_CLIP, 1+RATIO_CLIP) * advantages.detach()\n",
        "        actor_loss = -torch.min(surr1, surr2)\n",
        "        critic_loss = 0.5 * advantages**2\n",
        "        loss = actor_loss + 0.5*critic_loss - 0.01 * new_entropy\n",
        "        policy_optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(new_model.parameters(), 1)\n",
        "        policy_optimizer.step()\n",
        "\n",
        "        predicted_features, target_features = rnd_model(next_states)\n",
        "        rnd_loss = (predicted_features - target_features).pow(2).sum(1) / 2\n",
        "\n",
        "        rnd_optimizer.zero_grad()\n",
        "        # print(rnd_loss.mean().item())\n",
        "        rnd_loss.mean().backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(new_model.parameters(), 1)\n",
        "        rnd_optimizer.step()\n",
        "\n",
        "    old_model.load_state_dict(new_model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PDtyr_9kjfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.next_states = []\n",
        "        self.rewards_extrinsic = []\n",
        "        self.rewards_intrinsic = []\n",
        "        self.actions = []\n",
        "        self.done_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdwXoofR3FLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hl9tustYZmC",
        "colab_type": "text"
      },
      "source": [
        "# 1. Add Clipping to gradients\n",
        "# 2. Increase the timesteps before making an update\n",
        "# 3. Tensorboard support?\n",
        "# 4. Add stack of frames to represent movement\n",
        "# 5. Add reward / observation normalization\n",
        "# 6. Add input normalization (is this needed after observation normalization?)\n",
        "# 7. Figure out NaN issue. Possibly due to high entropy loss coefficient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqAo1lhbj0G-",
        "colab_type": "code",
        "outputId": "bba7fe09-c451-4821-bba2-2e8963deb944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f349cd696eae4548ae81876018b87fc7",
            "366bfab6750e4f1580598b3bb1d62f97",
            "0a3c189d8f89480eb7d77730d9512c1c",
            "c79f4606d482406bb8a2cec42b0c6498",
            "33aea79bb3354326968bc76ecd5eea4b",
            "3762d32f146b4726830f85ac166d021a",
            "1568a33943a14ea997d35db3427b62b4",
            "0a55a1a99d454e92b5be5e0f1f142de0"
          ]
        }
      },
      "source": [
        "NUM_EPISODES = 3000\n",
        "MAX_EPISODE_STEPS = 15000\n",
        "UPDATE_TIME_STEP = 2000\n",
        "\n",
        "env = gym.make('Freeway-v0')\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "NUM_STATE_DIMENSIONS = env.observation_space.shape[0]\n",
        "\n",
        "torch.manual_seed(2048)\n",
        "np.random.seed(2048)\n",
        "\n",
        "episodic_rewards = []\n",
        "\n",
        "iteration_memory = Memory()\n",
        "\n",
        "old_policy_net = PolicyNetwork(NUM_STATE_DIMENSIONS, NUM_ACTIONS).cuda()\n",
        "new_policy_net = PolicyNetwork(NUM_STATE_DIMENSIONS, NUM_ACTIONS).cuda()\n",
        "rnd_network = RNDNetwork().cuda()\n",
        "\n",
        "old_policy_net.load_state_dict(new_policy_net.state_dict())\n",
        "\n",
        "policy_optimizer = torch.optim.Adam(new_policy_net.parameters(), lr=1e-3)\n",
        "rnd_optimizer = torch.optim.Adam(rnd_network.parameters(), lr=1e-3)\n",
        "\n",
        "iterations = 1\n",
        "\n",
        "for episode in tqdm(range(NUM_EPISODES)):\n",
        "    current_state = env.reset()\n",
        "    current_state = torch.from_numpy(rgb2gray(current_state) / 255.).float().unsqueeze(0).unsqueeze(0).cuda()\n",
        "    reward_list = []\n",
        "    for iteration in range(MAX_EPISODE_STEPS):\n",
        "        # print(_)\n",
        "        chosen_action = old_policy_net.get_action(current_state)\n",
        "        next_state, reward_extrinsic, done, _ = env.step(chosen_action)\n",
        "        next_state = torch.from_numpy(rgb2gray(next_state)).float().unsqueeze(0).unsqueeze(0).cuda()\n",
        "        \n",
        "        reward_intrinsic = rnd_network.get_intrinsic_reward(next_state)\n",
        "        # print(reward_intrinsic.shape)\n",
        "        reward_list.append(reward_extrinsic)\n",
        "\n",
        "        iteration_memory.states.append(current_state)\n",
        "        iteration_memory.next_states.append(next_state)\n",
        "        iteration_memory.actions.append(chosen_action)\n",
        "        iteration_memory.rewards_extrinsic.append(reward_extrinsic)\n",
        "        iteration_memory.rewards_intrinsic.append(reward_intrinsic)\n",
        "        iteration_memory.done_list.append(done)\n",
        "\n",
        "        bootstrap_state = next_state\n",
        "        current_state = next_state\n",
        "\n",
        "        if len(iteration_memory.states) == 32:\n",
        "            states_tensor = torch.cat(iteration_memory.states, dim=0)\n",
        "            # print(iteration_memory.rewards_intrinsic)\n",
        "            next_states_tensor = torch.cat(iteration_memory.next_states, dim=0)\n",
        "            action_tensor = torch.Tensor(iteration_memory.actions).long().cuda()\n",
        "            rewards_intrinsic = iteration_memory.rewards_intrinsic\n",
        "            rewards_extrinsic = iteration_memory.rewards_extrinsic\n",
        "\n",
        "            done_list = np.array(iteration_memory.done_list).astype(int)\n",
        "            make_ppo_update(states_tensor, next_states_tensor, action_tensor, rewards_extrinsic, rewards_intrinsic, done_list, bootstrap_state, old_policy_net, new_policy_net, rnd_network, policy_optimizer, rnd_optimizer)\n",
        "            iteration_memory.clear()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episodic_rewards.append(sum(reward_list))\n",
        "    print(\"Episode length - {} with reward - {}\".format(iteration, episodic_rewards[-1]))\n",
        "    if episode % 100 == 0:\n",
        "        episodic_rewards_smoothed = np.convolve(episodic_rewards, np.ones((100,))/100, mode='valid')\n",
        "        print(\"reward at step - {} - {}\".format(episode, episodic_rewards_smoothed[-1]))\n",
        "        if episodic_rewards_smoothed[-1] > 200:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f349cd696eae4548ae81876018b87fc7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=3000), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Episode length - 2728 with reward - 20.0\n",
            "reward at step - 0 - 0.2\n",
            "Episode length - 2728 with reward - 0.0\n",
            "Episode length - 2694 with reward - 20.0\n",
            "Episode length - 2744 with reward - 21.0\n",
            "Episode length - 2736 with reward - 21.0\n",
            "Episode length - 2714 with reward - 21.0\n",
            "Episode length - 2742 with reward - 21.0\n",
            "Episode length - 2729 with reward - 21.0\n",
            "Episode length - 2743 with reward - 21.0\n",
            "Episode length - 2719 with reward - 21.0\n",
            "Episode length - 2744 with reward - 24.0\n",
            "Episode length - 2734 with reward - 21.0\n",
            "Episode length - 2722 with reward - 21.0\n",
            "Episode length - 2715 with reward - 23.0\n",
            "Episode length - 2717 with reward - 21.0\n",
            "Episode length - 2743 with reward - 23.0\n",
            "Episode length - 2733 with reward - 21.0\n",
            "Episode length - 2707 with reward - 21.0\n",
            "Episode length - 2745 with reward - 21.0\n",
            "Episode length - 2733 with reward - 21.0\n",
            "Episode length - 2692 with reward - 21.0\n",
            "Episode length - 2715 with reward - 21.0\n",
            "Episode length - 2733 with reward - 21.0\n",
            "Episode length - 2732 with reward - 21.0\n",
            "Episode length - 2721 with reward - 21.0\n",
            "Episode length - 2706 with reward - 21.0\n",
            "Episode length - 2753 with reward - 1.0\n",
            "Episode length - 2730 with reward - 0.0\n",
            "Episode length - 2728 with reward - 0.0\n",
            "Episode length - 2738 with reward - 0.0\n",
            "Episode length - 2750 with reward - 0.0\n",
            "Episode length - 2738 with reward - 0.0\n",
            "Episode length - 2729 with reward - 0.0\n",
            "Episode length - 2735 with reward - 0.0\n",
            "Episode length - 2714 with reward - 0.0\n",
            "Episode length - 2719 with reward - 0.0\n",
            "Episode length - 2738 with reward - 0.0\n",
            "Episode length - 2743 with reward - 0.0\n",
            "Episode length - 2725 with reward - 0.0\n",
            "Episode length - 2730 with reward - 0.0\n",
            "Episode length - 2715 with reward - 0.0\n",
            "Episode length - 2726 with reward - 0.0\n",
            "Episode length - 2729 with reward - 0.0\n",
            "Episode length - 2740 with reward - 0.0\n",
            "Episode length - 2733 with reward - 0.0\n",
            "Episode length - 2730 with reward - 0.0\n",
            "Episode length - 2728 with reward - 23.0\n",
            "Episode length - 2719 with reward - 0.0\n",
            "Episode length - 2726 with reward - 0.0\n",
            "Episode length - 2746 with reward - 0.0\n",
            "Episode length - 2748 with reward - 0.0\n",
            "Episode length - 2741 with reward - 0.0\n",
            "Episode length - 2726 with reward - 0.0\n",
            "Episode length - 2736 with reward - 0.0\n",
            "Episode length - 2734 with reward - 0.0\n",
            "Episode length - 2727 with reward - 0.0\n",
            "Episode length - 2735 with reward - 0.0\n",
            "Episode length - 2724 with reward - 0.0\n",
            "Episode length - 2741 with reward - 0.0\n",
            "Episode length - 2727 with reward - 0.0\n",
            "Episode length - 2732 with reward - 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRHYrLssSeJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%debug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DzHLDCEXKC_",
        "colab_type": "code",
        "outputId": "7d8450e1-931a-4b99-e16d-0b29d9584b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "episodic_rewards"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Cgtueqpidv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}